{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe7a88f",
   "metadata": {},
   "source": [
    "#### 1. **Reading CSVs and Feature Extraction**\n",
    "- Reading the features CSV: O(n·d), where n = number of samples, d = number of features per sample.\n",
    "- Column selection and checking: O(d).\n",
    "\n",
    "#### 2. **Shapiro-Wilk Normality Test**\n",
    "- Outer loop: 10 classes (constant, call it c).\n",
    "- Inner loop: d features.\n",
    "- For each, `shapiro` runs on up to n_c samples (n_c ≤ n), but in practice, c ≪ n, d ≪ n, so this is **O(d·c·k)** where k is average samples per class (k = n/c). Thus, O(n·d).\n",
    "\n",
    "#### 3. **Kruskal-Wallis Test**\n",
    "- Loop over d features.\n",
    "- For each, groupby by class (O(n)), then `kruskal` compares c groups (each ~k samples).\n",
    "- Complexity per feature: O(n) (to create groups), total O(n·d).\n",
    "\n",
    "#### 4. **Category Means and Overall Means**\n",
    "- Groupby and mean over n samples and d features: O(n·d).\n",
    "\n",
    "#### 5. **Scatter Matrices (Within- and Between-class)**\n",
    "- Within-class: For each class (c), difference and dot products over d features for each of n samples: O(n·d^2).\n",
    "- Between-class: For each class, difference and dot products over d: O(c·d^2).\n",
    "\n",
    "#### 6. **Fisher's Discriminant Ratio**\n",
    "- Matrix inversion of d×d matrix: O(d^3).\n",
    "- Matrix multiplication: O(d^3).\n",
    "- For small d (few features), this is fast; for large d, this can dominate.\n",
    "\n",
    "#### 7. **AUC Calculation**\n",
    "- For each class (c) and feature (d): computes AUC over n samples.\n",
    "- O(c·d·n).\n",
    "\n",
    "#### 8. **Filtering Classes with <2 Samples**\n",
    "- Value counts and masking: O(n).\n",
    "\n",
    "#### 9. **Train/Test Split and Random Forest**\n",
    "- `train_test_split`: O(n).\n",
    "- Training RandomForestClassifier: Let t be the number of trees, m samples for training, f features per tree, depth h. Empirically, each tree is O(f·m·log m), so total O(t·f·m·log m).\n",
    "- Prediction: O(t·f·m_test·log m), m_test is test set size.\n",
    "\n",
    "#### 10. **Metrics, Report, DataFrame Export**\n",
    "- All metrics: O(n).\n",
    "- Exporting CSVs: O(n·d).\n",
    "\n",
    "#### 11. **KDE and Boxplots**\n",
    "- For each feature (d), plotting KDE/boxplot:\n",
    "    - Seaborn's `kdeplot` is O(n) for that feature.\n",
    "    - So, O(n·d) for all plots.\n",
    "- Saving plots: O(1) per plot.\n",
    "\n",
    "#### **Overall Complexity**\n",
    "- **O(n·d + d^3 + t·f·m·log m)**, where\n",
    "    - n: samples\n",
    "    - d: features\n",
    "    - t: trees (random forest)\n",
    "    - f: features per tree (usually d or sqrt(d))\n",
    "    - m: train set size\n",
    "\n",
    "- For small d (common in feature extraction), script is dominated by the number of samples and forest training.\n",
    "- For large d (high-dimensional data), Fisher's discriminant and scatter matrices can dominate.\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Step                   | Big O             |\n",
    "|------------------------|-------------------|\n",
    "| CSV Reading            | O(n·d)            |\n",
    "| Shapiro/Kruskal        | O(n·d)            |\n",
    "| Means/Groupby          | O(n·d)            |\n",
    "| Scatter Matrices       | O(n·d^2)          |\n",
    "| Fisher Ratio           | O(d^3)            |\n",
    "| AUC                    | O(n·d)            |\n",
    "| Filtering              | O(n)              |\n",
    "| Train/Test Split       | O(n)              |\n",
    "| Random Forest Train    | O(t·f·m·log m)    |\n",
    "| Metrics/Export         | O(n·d)            |\n",
    "| KDE/Boxplots           | O(n·d)            |\n",
    "\n",
    "- **Dominant terms for practical cases**: O(n·d) or O(t·f·m·log m).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
